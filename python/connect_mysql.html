<!DOCTYPE html>
<html>
<head>
	<title>python coding</title>
	<meta charset="utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	<link rel="stylesheet" type="text/css" href="../css/index.css">
</head>
<body>
<header>
	<div class="info">
		<div class="blog-photo"><img src="../images/me.jpg"></div>
		<div class="blog-intro">
			<p class="blog-name">初木禾君</p>
			<p class="blog-university">Tianjin University</p>
		</div>
	</div>
</header>
<div class="container">
<h1>使用python 做一些cool的事情</h1>
<article>
	<h2>使用python连接mysql数据库并读取数据库数据</h2>
	<pre>

import pymysql
conn = pymysql.connect(host="127.0.0.1",user = 'root',passwd = None,db = 'mysql')
cur = conn.cursor()
cur.execute("USE scraping")
cur.execute("SELECT * FROM pages WHERE id=1")
print (cur.fetchone())
cur.close()
conn.close()
	
	</pre>
</article>
<article>
	<h2>将采集的数据写入mysql数据库</h2>
	<pre>
#-*- coding:utf-8 -*-
import pymysql
import urllib
import urllib2
import re
import datetime
import random
from bs4 import BeautifulSoup
import sys

reload(sys)

sys.setdefaultencoding('utf-8')
user_agent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4"
headers = { 'User-Agent' : user_agent }
conn = pymysql.connect(host="127.0.0.1",user = 'root',passwd = None,db = 'mysql')
cur = conn.cursor()
random.seed(datetime.datetime.now())
cur.execute("USE webscraping")
def store(title,content):
	cur.execute("INSERT INTO pages(title,content) VALUES (\"%s\",\"%s\")",(title,content))
	cur.connection.commit()

def getLinks(articleUrl):
	html = urllib.urlopen("https://en.wikipedia.org"+articleUrl)
	bsObj = BeautifulSoup(html,"html.parser")
	title = bsObj.find("h1").get_text()
	content = bsObj.find("div",{"id":"mw-content-text"}).find("p").get_text()
	store(title,content)
	return bsObj.find("div",{"id":"bodyContent"}).findAll("a",href=re.compile("^(/wiki/)((?!:).)*$"))
links = getLinks("/wiki/KEVIN_Bacon")
try:
	while len(links)>0:
		newArticle = links[random.randint(0,len(links)-1)].attrs["href"]
		print (newArticle)
		links = getLinks(newArticle)
finally:
	cur.close()
	conn.close()
	</pre>
</article>

</div>
</body>
</html>